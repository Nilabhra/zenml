# Creating your first pipeline

{ "cells": \[ { "cell\_type": "markdown", "metadata": {}, "source": \[ "\# Creating your first pipeline" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "This is the notebook version of our [Quickstart](../steps/quickstart.md)! Our goal here is to help you to get the first practical experience with our tool and give you a brief overview on some basic functionalities of **ZenML**.\n", "\n", "In this example, we will create and run a simple pipeline featuring a local CSV dataset and a basic feedforward neural network and run it in our local environment. If you want to run this notebook in an interactive environment, feel free to [run it in a Google Colab](https://colab.research.google.com/github/maiot-io/zenml/blob/main/docs/book/tutorials/simple-classification.ipynb)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\# First things first..." \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "You can install **ZenML** through:\n", "\n", "`\n", "pip install zenml\n", "`\n", "\n", "Once the installation is completed, you can go ahead and create your first **ZenML** repository for your project. As **ZenML** repositories are built on top of Git repositories, you can create yours in a desired empty directory through:\n", "\n", "`\n", "git init\n", "zenml init\n", "`\n", "\n", "Now, the setup is completed. For the next steps, just make sure that you are executing the code within your **ZenML** repository." \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\# Creating the pipeline" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "Once you set everything up, we can start our tutorial. The first step is to create an instance of a pipeline. **ZenML** comes equipped with different types of pipelines, but for this example we will be using the most classic one, namely a `TrainingPipeline`. \n", "\n", "While creating your pipeline, you can give it a name and use that name to reference the pipeline later." \] }, { "cell\_type": "code", "execution\_count": null, "metadata": {}, "outputs": \[\], "source": \[ "from zenml.pipelines import TrainingPipeline\n", "\n", "training\_pipeline = TrainingPipeline\(name='QuickstartPipeline'\)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "In a **ZenML** `TrainingPipeline`, there is a fixed set of steps representing the processes, which can be found in any machine learning workflow. These steps include:\n", " \n", "1. **Split**: responsible for splitting your dataset into smaller datasets such as train, eval, etc.\n", "2. **Transform**: responsible for the preprocessing of your data\n", "3. **Train**: responsible for the model creation and training process\n", "4. **Evaluate**: responsible for the evaluation of your results" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\# Creating a datasource" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "However, before we dive into the aforementioned steps, let's briefly talk about our dataset.\n", "\n", "For this quickstart, we will be using the _Pima Indians Diabetes Dataset_ and on it, we will train a model which will aim to predict whether a person has diabetes based on diagnostic measures.\n", "\n", "In order to be able to use this dataset \(which is currently in CSV format\) in your **ZenML** pipeline, we first need to create a `datasource`. **ZenML** has built-in support for various types of datasources and for this example you can use the `CSVDatasource`. All you need to provide is a `name` for the datasource and the `path` to the CSV file." \] }, { "cell\_type": "code", "execution\_count": null, "metadata": {}, "outputs": \[\], "source": \[ "from zenml.datasources import CSVDatasource\n", "\n", "ds = CSVDatasource\(name='Pima Indians Diabetes Dataset', \n", " path='gs://zenml\_quickstart/diabetes.csv'\)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "Once you are through, you will have created a tracked and versioned datasource and you can use this datasource in any pipeline. Go ahead and add it to your pipeline." \] }, { "cell\_type": "code", "execution\_count": null, "metadata": {}, "outputs": \[\], "source": \[ "training\_pipeline.add\_datasource\(ds\)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\# Configuring the split" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "Now, let us get back to the **four** essential steps where the first step is the **Split**. \n", "\n", "For the sake of simplicity in this tutorial, we will be using a completely random `70-30` split into a train and evaluation dataset." \] }, { "cell\_type": "code", "execution\_count": null, "metadata": {}, "outputs": \[\], "source": \[ "from zenml.steps.split import RandomSplit\n", "\n", "training\_pipeline.add\_split\(RandomSplit\(split\_map={'train': 0.7, \n", " 'eval': 0.3}\)\)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "Keep in mind, in a more complicated example, it might be necessary to apply a different splitting strategy. For these cases, you can use the other built-in split configuration **ZenML** offers or even implement your own custom logic into the split step." \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\# Handling data preprocessing" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "The next step is to configure the step **Transform**, the data preprocessing.\n", "\n", "For this example, we will use the built-in `StandardPreprocesser`. It handles the feature selection and has sane defaults of preprocessing behaviour for each data type, such as stardardization for numerical features or vocabularization for non-numerical features.\n", "\n", "In order to use it, you need to provide a list of feature names and a list of label names. Moreover, if you do not want it use the default transformation for a feature or you want to overwrite it with a different preprocessing method, this is also possible as we do in this example." \] }, { "cell\_type": "code", "execution\_count": null, "metadata": {}, "outputs": \[\], "source": \[ "from zenml.steps.preprocesser import StandardPreprocesser\n", "\n", "training\_pipeline.add\_preprocesser\(\n", " StandardPreprocesser\(\n", " features=\['times\_pregnant', \n", " 'pgc', \n", " 'dbp', \n", " 'tst', \n", " 'insulin', \n", " 'bmi',\n", " 'pedigree', \n", " 'age'\],\n", " labels=\['has\_diabetes'\],\n", " overwrite={'has\_diabetes': {\n", " 'transform': \[{'method': 'no\_transform', \n", " 'parameters': {}}\]}}\)\)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "Much like the splitting process, you might want to work on cases, where the capabilities of the `StandardPreprocesser` do not match your task at hand. In this case, you can create your own custom preprocessing step, but we will go into that topic in a different tutorial." \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\# Training your model" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "As the data is now ready, we can move onto the step **Train**, the model creation and training.\n", "\n", "For this quickstart, we will be using the simple built-in `FeedForwardTrainer` step and as the name suggests, it represents a feedforward neural network, which is configurable through a set of variables. " \] }, { "cell\_type": "code", "execution\_count": null, "metadata": {}, "outputs": \[\], "source": \[ "from zenml.steps.trainer import TFFeedForwardTrainer\n", "\n", "training\_pipeline.add\_trainer\(TFFeedForwardTrainer\(loss='binary\_crossentropy',\n", " last\_activation='sigmoid',\n", " output\_units=1,\n", " metrics=\['accuracy'\],\n", " epochs=20\)\)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "Of course, not every single machine learning problem is solvable by a simple feedforward neural network and most of the time, they will require a model which is tailored to the corresponding problem. That is why we created an interface where the users can implement their own custom models and integrate it in a trainer step. However this approach is not within the scope of this tutorial and you can learn more about it in our docs and the upcoming tutorials." \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\# Evaluation of the results" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "The last step to configure in our pipeline is the **Evaluate**.\n", "\n", "For this example, we will be using the built-in `TFMAEvaluator` which uses [Tensorflow Model Analysis](https://www.tensorflow.org/tfx/model_analysis/get_started) to compute metrics based on your results \(possibly within slices\)." \] }, { "cell\_type": "code", "execution\_count": null, "metadata": {}, "outputs": \[\], "source": \[ "from zenml.steps.evaluator import TFMAEvaluator\n", "\n", "training\_pipeline.add\_evaluator\(\n", " TFMAEvaluator\(slices=\[\['has\_diabetes'\]\],\n", " metrics={'has\_diabetes': \['binary\_crossentropy',\n", " 'binary\_accuracy'\]}\)\)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\# Running your pipeline" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "Now that everything is set, go ahead and run the pipeline, thus your steps." \] }, { "cell\_type": "code", "execution\_count": null, "metadata": {}, "outputs": \[\], "source": \[ "training\_pipeline.run\(\)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "With the execution of the pipeline, you should see the logs informing you about each step along the way. In more detail, you should first see that your dataset will is ingested through the component _DataGen_ and then split by the component _SplitGen_. Afterwards data preprocessing will take place with the component _Transform_ and will lead to the main training component _Trainer_. Ultimately, the results will be evaluated by the component _Evaluator_." \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\# Post-training functionalities" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "Once the training pipeline is finished, you can check the outputs of your pipeline in different ways." \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\#\# Dataset" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "As the data is now ingested, you can go ahead and take a peek into your dataset. You can achieve this by simply getting the datasources registered to your repository and calling the method `sample_data`." \] }, { "cell\_type": "code", "execution\_count": null, "metadata": {}, "outputs": \[\], "source": \[ "from zenml.repo import Repository\n", "\n", "repo = Repository.get\_instance\(\)\n", "datasources = repo.get\_datasources\(\)\n", "\n", "datasources\[0\].sample\_data\(\)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\#\# Statistics" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "Furthermore, you can check the statistics which are yielded by your datasource and split configuration through the method `view_statistics`. By using the `magic` flag, we can even achieve this right here in this notebook." \] }, { "cell\_type": "code", "execution\_count": null, "metadata": {}, "outputs": \[\], "source": \[ "training\_pipeline.view\_statistics\(magic=True\)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "\#\#\# Evaluate" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "On the other hand, if you want to evalaute the results of your training process you can use the `evaluate` method of your pipeline. \n", "\n", "Much like the `view_statistics`, if you execute `evaluate` with the `magic` flag, it will help you continue in this notebook and generate two new cells, each set up with a different evaluation tool:\n", "\n", "1. **Tensorboard** can help you to understand the behaviour of your model during the training session\n", "2. **TFMA** or **tensorflow\_model\_analysis** can help you assess your already trained model based on given metrics and slices on the evaluation dataset \n", "\n", "_Note_: if you want to see the sliced results, comment in the last line and adjust it according to the slicing column. In the end it should look like this:\n", "`\n", "tfma.view.render_slicing_metrics(evaluation, slicing_column='has_diabetes')\n", "`" \] }, { "cell\_type": "code", "execution\_count": null, "metadata": {}, "outputs": \[\], "source": \[ "training\_pipeline.evaluate\(magic=True\)" \] }, { "cell\_type": "markdown", "metadata": {}, "source": \[ "... and this it it for the quickstart. If you came here without a hiccup, you must have successly installed ZenML, set up a ZenML repo, registered a new datasource, configured a training pipeline, executed it locally and evaluated the results. And, this is just the tip of the iceberg on the capabilities of **ZenML**. \n", "\n", "However, if you had a hiccup or you have some suggestions/questions regarding our framework, you can always check our [docs](https://docs.zenml.io/) or our [github](https://github.com/maiot-io/zenml) or even better join us on our [Slack](https://zenml.io/slack-invite) channel.\n", "\n", "Cheers!" \] } \], "metadata": { "kernelspec": { "display\_name": "Python 3", "language": "python", "name": "python3" }, "language\_info": { "codemirror\_mode": { "name": "ipython", "version": 3 }, "file\_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert\_exporter": "python", "pygments\_lexer": "ipython3", "version": "3.6.9" } }, "nbformat": 4, "nbformat\_minor": 4 }

