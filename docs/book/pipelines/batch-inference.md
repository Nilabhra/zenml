# Batch Inference \[WIP\]

BatchInference pipelines are meant to run batch jobs to run a bunch of data through a trained ML model.

The current version of `zenml` does not fully support BatchInference as of this moment.

If you need this functionality earlier, then ping us on our [Slack](https://zenml.io/slack-invite) or [create an issue on GitHub](https://https://github.com/maiot-io/zenml) so that we know about it!

